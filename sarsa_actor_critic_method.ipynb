{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as anim\n",
    "import typing as tp\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "SEED = 563575\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED+1)\n",
    "torch.manual_seed(SEED+2)\n",
    "torch.use_deterministic_algorithms(mode=True, warn_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_environment(env:gym.Env, figsize:tuple[int, int]=(5, 4)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    img = env.render()\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    return img\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames:list, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    animation = anim.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    return animation\n",
    "\n",
    "def show_one_episode(action_sampler:tp.Callable, n_max_steps=500, repeat=False):\n",
    "    frames = []\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    obs, info = env.reset()\n",
    "    with torch.no_grad():\n",
    "        for step in range(n_max_steps):\n",
    "            frames.append(env.render())\n",
    "            action = action_sampler(obs).cpu().item()\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            if done or truncated:\n",
    "                print(\"done at step\", step+1)\n",
    "                break\n",
    "    env.close()\n",
    "    return plot_animation(frames, repeat=repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineDecayWithWarmup:\n",
    "    def __init__(\n",
    "        self,\n",
    "        warmup_steps:int,\n",
    "        max_learning_rate:float,\n",
    "        decay_steps:int,\n",
    "        min_learning_rate:float\n",
    "    ):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.max_learning_rate = max_learning_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "\n",
    "    def __call__(self, step):\n",
    "        # linear warmup for warmup_steps steps\n",
    "        if step < self.warmup_steps:\n",
    "            return self.max_learning_rate * step / self.warmup_steps\n",
    "        # if it > decay_steps, return min learning rate\n",
    "        if step > self.decay_steps:\n",
    "            return self.min_learning_rate\n",
    "        # in between, use cosine decay down to min learning rate\n",
    "        decay_ratio = (step - self.warmup_steps) / (self.decay_steps - self.warmup_steps)\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "        return self.min_learning_rate + coeff * (self.max_learning_rate - self.min_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00118216,  0.04504637, -0.03558404,  0.04486495], dtype=float32),\n",
       " (4,),\n",
       " {})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "obs, info = env.reset(seed=1)\n",
    "obs, obs.shape, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole Environment\n",
    "* GOAL: Keep the pole upright for as long as possible\n",
    "```python\n",
    "Actions:\n",
    "    left: 0\n",
    "    right: 1\n",
    "\n",
    "Observation Space: (4,)\n",
    "```\n",
    "![image.png](images/carpole_info.png)\n",
    "* The cart x-position (index 0) can be take values between (-4.8, 4.8), but the episode terminates if the cart leaves the (-2.4, 2.4) range.\n",
    "* The pole angle can be observed between (-.418, .418) radians (or ±24°), but the episode terminates if the pole angle is not in the range (-.2095, .2095) (or ±12°)\n",
    "\n",
    "## Rewards\n",
    "* Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted\n",
    "\n",
    "## Episode End\n",
    "* The episode ends if any one of the following occurs:\n",
    "    * Termination: Pole Angle is greater than ±12°\n",
    "    * Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "    * Truncation: Episode length is greater than 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic method using State Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class config:\n",
    "    num_episodes:int = 2000\n",
    "    num_over_to_average_grads:int = 32\n",
    "    max_steps_per_episode:int = 500\n",
    "    \n",
    "    gamma:float = 0.99\n",
    "    \n",
    "    actor_lr:float = 0.01\n",
    "    critic_lr:float = 0.01\n",
    "\n",
    "    weight_decay:float = 0.0 # if using prelu activation, weight decay for that param is not good\n",
    "    batch_size:int = 64\n",
    "    constlr_steps:int = int(num_episodes*0.4375)\n",
    "\n",
    "    device:torch.device = torch.device(\"cuda\" if False else \"cpu\") # cpu good for very very small models\n",
    "    dtype:torch.dtype = torch.float32 if \"cpu\" in device.type else torch.float16\n",
    "\n",
    "    autocast:torch.autocast = torch.autocast(\n",
    "        device_type=device.type, dtype=dtype, enabled=\"cuda\" in device.type\n",
    "    )\n",
    "    logging_interval:int = 1\n",
    "\n",
    "    generator:torch.Generator = torch.Generator(device=device).manual_seed(SEED+343434)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, num_inputs:int, num_hidden:int, num_outputs:int):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.last_act_func = nn.Sigmoid() if num_outputs == 1 else nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x:Tensor): # (B, num_inputs=4)\n",
    "        x = self.relu(self.linear1(x)) # (B, num_hidden=8)\n",
    "        x = self.last_act_func(self.linear2(x)) # (B, num_outputs=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    def __init__(self, num_inputs:int, num_hidden:int):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(num_hidden, 1)\n",
    "\n",
    "    def forward(self, x:Tensor): # (B, num_inputs=4)\n",
    "        x = self.relu(self.linear1(x)) # (B, num_hidden=8)\n",
    "        x = self.linear2(x) # (B, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Structure\n",
    "* Make Actor critic models\n",
    "* Make Training Loop\n",
    "  * Loop over episodes\n",
    "    * Loop over steps (till done)\n",
    "      * Sample actions from policy, Observe next state and reward\n",
    "      * Input state to Value/Critic Model to get the val\n",
    "      * Calculate TD-Error\n",
    "      * Optimize both Actor and Critic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in\n",
      "\tActor: 49\n",
      "\tCritic: 49\n"
     ]
    }
   ],
   "source": [
    "actor = Policy(\n",
    "    num_inputs=env.observation_space.shape[0],    # 4\n",
    "    num_hidden=env.observation_space.shape[0]*2,   # 8\n",
    "    num_outputs=1 if env.action_space.n == 2 else env.action_space.n # (left, right)\n",
    "); # actor.compile()\n",
    "actor_opt = torch.optim.NAdam(\n",
    "    actor.parameters(),\n",
    "    lr=config.actor_lr,\n",
    "    weight_decay=config.weight_decay,\n",
    "    maximize=True\n",
    ")\n",
    "\n",
    "critic = Value(\n",
    "    num_inputs=env.observation_space.shape[0],\n",
    "    num_hidden=env.observation_space.shape[0]*2\n",
    "); # critic.compile()\n",
    "critic_opt = torch.optim.NAdam(\n",
    "    critic.parameters(),\n",
    "    lr=config.critic_lr,\n",
    "    weight_decay=config.weight_decay,\n",
    "    maximize=False\n",
    ")\n",
    "\n",
    "get_lr = CosineDecayWithWarmup(\n",
    "    warmup_steps=1,\n",
    "    max_learning_rate=config.actor_lr,\n",
    "    decay_steps=config.num_episodes,\n",
    "    min_learning_rate=config.actor_lr*0.1\n",
    ")\n",
    "print(\"Number of parameters in\\n\\tActor: {}\\n\\tCritic: {}\".format(\n",
    "    sum(p.numel() for p in actor.parameters()), \n",
    "    sum(p.numel() for p in critic.parameters())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/actor_critic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_actions_from_actor(state:Tensor, B:int=1): # sample actions from actor given state\n",
    "    left_proba = actor(state) # (B, 1)\n",
    "    action:Tensor = torch.rand((B, 1), device=config.device) > left_proba # If `left_proba` is high, then `action` will most likely be `False` or 0, which means left\n",
    "    target = (~action).float() # 1 if action==0 (left) ; 0 elif action==1 (right)\n",
    "    actor_loss = (torch.log(left_proba).mul(target) + torch.log(1-left_proba).mul(1.0 - target)).mean() # GRADIENT ASCENT so no -ve sign\n",
    "    actor_loss.backward()\n",
    "    actor_grads = [p.grad for p in actor.parameters()]; actor.zero_grad()\n",
    "    return action.float(), actor_grads # 0 for left, 1 for right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m actor_grad_list, critic_grad_list \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(actor_avg_over_grads, critic_avg_over_grads):\n\u001b[1;32m     39\u001b[0m         apply_grads_actor_critic(\n\u001b[0;32m---> 40\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mactor_grad_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), \n\u001b[1;32m     41\u001b[0m             torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m(critic_grad_list[\u001b[38;5;241m0\u001b[39m]))))\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     42\u001b[0m         )\n\u001b[1;32m     45\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime(); dt \u001b[38;5;241m=\u001b[39m t1 \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| Step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mnum_episodes\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m || Episode Length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<6.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m || actor_lr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactor_lr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<12e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | critic_lr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcritic_lr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<12e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m || dt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<5.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms |\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got tuple"
     ]
    }
   ],
   "source": [
    "def apply_grads_actor_critic(actor_grads:list[Tensor], critic_grads:Tensor):\n",
    "    for p1, grad1, p2, grad2 in zip(actor.parameters(), actor_grads, critic.parameters(), critic_grads):\n",
    "        p1.grad = grad1\n",
    "        p2.grad = grad2\n",
    "    actor_opt.step(); actor.zero_grad()\n",
    "    critic_opt.step(); critic.zero_grad()\n",
    "\n",
    "actor.zero_grad(); critic.zero_grad()\n",
    "episode_lens = []\n",
    "for episode in range(config.num_episodes):\n",
    "    state, info = env.reset(); state = torch.as_tensor(state, device=config.device).unsqueeze(0)\n",
    "    t0 = time.time()\n",
    "    \n",
    "    lr = get_lr(episode)\n",
    "    actor_lr = lr  # config.actor_lr\n",
    "    critic_lr = lr    # config.critic_lr\n",
    "    for _ in range(config.num_over_to_average_grads):\n",
    "        actor_avg_over_grads:list[list[list[Tensor]]] = [[] for _ in range(config.max_steps_per_episode)]\n",
    "        critic_avg_over_grads:list[list[list[Tensor]]] = [[] for _ in range(config.max_steps_per_episode)]\n",
    "        for time_step in range(0, config.max_steps_per_episode):\n",
    "            action, actor_grads = sample_actions_from_actor(state)\n",
    "            \n",
    "            bef_state_value = critic(state)\n",
    "            state, reward, done, truncated, _ = env.step(int(action.item())); state = torch.as_tensor(state, device=config.device).unsqueeze(0)\n",
    "            aft_state_value:Tensor = torch.tensor(0.0, device=config.device).view(1, 1) if done else critic(state)\n",
    "            td:Tensor = (reward + config.gamma*aft_state_value - bef_state_value).squeeze() # (,)\n",
    "\n",
    "            critic_loss = td.square().mean(); critic_loss.backward() # MSE Loss\n",
    "            critic_grads = [p.grad for p in critic.parameters()]; critic.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                actor_grads = [actor_grad.mul(td).mul(config.gamma**time_step) for actor_grad in actor_grads]\n",
    "            actor_avg_over_grads[time_step].append(actor_grads)\n",
    "            critic_avg_over_grads[time_step].append(critic_grads)\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        for actor_grad_list, critic_grad_list in zip(actor_avg_over_grads, critic_avg_over_grads):\n",
    "            apply_grads_actor_critic(\n",
    "                torch.stack(list(zip(*(actor_grad_list[0])))).mean(dim=0), # ERROR HERE\n",
    "                torch.stack(list(zip(*(critic_grad_list[0])))).mean(dim=0)\n",
    "            )\n",
    "\n",
    "\n",
    "    t1 = time.time(); dt = t1 - t0\n",
    "    print(f\"| Step: {episode:<4}/ {config.num_episodes:<4} || Episode Length {time_step+1:<6.2f} || actor_lr: {actor_lr:<12e} | critic_lr: {critic_lr:<12e} || dt: {dt:<5.2f}s |\")\n",
    "    episode_lens.append(time_step+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(*actor_grad_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_lens)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Average Episode Length\")\n",
    "plt.yticks(range(0, 501, 50))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
